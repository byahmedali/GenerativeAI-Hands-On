{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e5b680-4e25-4b75-aa64-5b8c64375544",
   "metadata": {},
   "source": [
    "# **Ollama Local Models**\n",
    "\n",
    "[Ollama](https://ollama.com/) allows you to run open-source large language models, such as Llama 2, locally.\n",
    "\n",
    "For a complete list of supported models and model variants, see the [Ollama model library](https://ollama.com/search).\n",
    "\n",
    "We will chat with locally running reasoning LLM, `deepseek-r1:1.5b` as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e326e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Greetings! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. I'm at your service and would be delighted to assist you with any inquiries or tasks you may have.\n",
      "</think>\n",
      "\n",
      "Greetings! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. I'm at your service and would be delighted to assist you with any inquiries or tasks you may have.\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "def ChatOllama(prompt):\n",
    "    response: ChatResponse = chat(model='deepseek-r1:1.5b',\n",
    "    messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': prompt,\n",
    "    }\n",
    "    ])\n",
    "    return response\n",
    "\n",
    "chat_response = ChatOllama(\"Introduce yourself.\")\n",
    "print(chat_response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5e20632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out how Ollama works. From what I understand, Ollama is a tool that lets me run open-source large language models like Llama 2. It sounds cool because it makes it easier for people to create AI models without having to build them from scratch.\n",
      "\n",
      "First, the input text mentions that Ollama allows running Llama 2 locally. That makes sense because I know other tools like ChatGPT do something similar, but I'm not exactly sure how they differ. Maybe they're all about running pre-trained models? Or is it more about training models myself?\n",
      "\n",
      "Then there's this part about the model library for documentation: [Ollama model library](https://ollama.com/search). That sounds like a resource where you can find different models and their variants. I guess that means if someone wants to try out Llama 2 or a specific variant, they can use this tool.\n",
      "\n",
      "The example given is running `deepseek-r1:1.5b`. Wait, what does the colon thing mean here? Is it a model identifier followed by some parameters? I think in AI models, sometimes you specify the model's capabilities like the number of attention heads or layers. Maybe that refers to different variants or modes of the Llama 2 model.\n",
      "\n",
      "So putting it all together, Ollama seems to be an accessible tool for running and experimenting with open-source language models. It simplifies things by letting users run pre-trained models locally, which is helpful for quick prototyping. The documentation resource helps users understand what they're working with, including different configurations or variants of the model.\n",
      "\n",
      "I wonder how easy it is to get started. Do I just need a basic setup like a computer and internet access? Probably not too much since Ollama is built on top of Python. I should check out some tutorials or documentation to learn more about how to use it properly.\n",
      "\n",
      "Also, are there any limitations to using Ollama? Maybe some models aren't supported, or there might be restrictions on how they can be used in certain environments. It's probably something that people will have to experiment with themselves after trying it out.\n",
      "\n",
      "I'm curious about the training process once I choose a model. How does that work? Do I just copy the model file and run it directly, or do I need to train it myself? The example given uses `deepseek-r1:1.5b`, which might be pre-trained, but if not, would Ollama require some setup?\n",
      "\n",
      "Overall, Ollama seems like a good tool for exploring and experimenting with AI models without getting bogged down in complex setups. It's user-friendly and accessible, making it easier to dive into language model development.\n",
      "</think>\n",
      "\n",
      "**Ollama Overview: A User-Friendly Tool for Open-Source Language Models**\n",
      "\n",
      "**Introduction**\n",
      "Ollama is an open-source platform designed to simplify the creation and experimentation with large language models (LLMs). Making AI models accessible through user-friendly tools, Ollama allows users to run pre-trained models locally, enabling quick prototyping without extensive setup.\n",
      "\n",
      "**Key Features and Functionality**\n",
      "\n",
      "1. **Running Local Models**\n",
      "   - Ollama facilitates running open-source LLMs like Llama 2 directly on a computer. This is beneficial for rapid development and experimentation.\n",
      "   - Example: Running `deepseek-r1:1.5b` with the command `ollama deepseek-r1:1.5b`.\n",
      "\n",
      "2. **Model Library**\n",
      "   - The [Ollama model library](https://ollama.com/search) provides documentation on various models and their variants, offering a resource for deeper exploration and customization.\n",
      "\n",
      "3. **User-Friendly Environment**\n",
      "   - Designed to be user-friendly, Ollama is accessible with basic setup (computers, internet access).\n",
      "\n",
      "**How It Works**\n",
      "\n",
      "- **Pretrained Models:** Ollama enables running existing LLMs like Llama 2 without manual training, ideal for quick development.\n",
      "- **Documentation:** The model library helps users understand their models, including different configurations or variants.\n",
      "\n",
      "**Limitations and Exploration**\n",
      "\n",
      "While user-friendly, Ollama has limitations such as supported models and environment requirements. Users may need experimentation to fully grasp how models are used and trained.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Ollama is an accessible tool for exploring AI models, offering simplicity and ease of use while providing documentation on models and their configurations. While it's a helpful starting point, users might require further exploration or setup depending on their needs.\n",
      "\n",
      "**Final Answer**\n",
      "Ollama is an accessible platform that allows running open-source LLMs locally, with documentation available to explore different models and variants. It's ideal for rapid prototyping but may need user experimentation to fully utilize its features.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\" [Ollama](https://ollama.com/) allows you to run open-source large language models, such as Llama 2, locally.\n",
    "\n",
    "For a complete list of supported models and model variants, see the [Ollama model library](https://ollama.com/search).\n",
    "\n",
    "We will chat with locally running `deepseek-r1:1.5b` as an example.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Explain the provided input text as you are teaching a 5 year old learning AI.\n",
    "Input_text: {input_text}\n",
    "\"\"\"\n",
    "\n",
    "response = ChatOllama(prompt)\n",
    "print(response.message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
